We utilize EVA-02 as the foundational model for OAD-VIT, a cutting-edge Transformer-based architecture designed to generate robust, language-aligned visual representations through masked image modeling. EVA-02 incorporates an enhanced plain Transformer structure and benefits from comprehensive pre-training with an open and accessible large-scale CLIP vision encoder. This combination enables EVA-02 to achieve superior results across a wide range of key visual tasks, outperforming previous state-of-the-art models, all while operating with significantly fewer parameters and computational resources.


## Get Started
- [Pre-training](asuka)
- [Image Classification](asuka)
- [Object Detection & Instance Segmentation](det)
- [Semantic Segmentation](seg)
- [CLIP](../EVA-CLIP)


## Acknowledgement
[EVA-01](https://github.com/baaivision/EVA/tree/master/EVA-01), [BEiT](https://github.com/microsoft/unilm/tree/master/beit), [BEiTv2](https://github.com/microsoft/unilm/tree/master/beit2), [CLIP](https://github.com/openai/CLIP), [MAE](https://github.com/facebookresearch/mae/), [timm](https://github.com/rwightman/pytorch-image-models), [DeepSpeed](https://github.com/microsoft/DeepSpeed), [Apex](https://github.com/NVIDIA/apex), [xFormer](https://github.com/facebookresearch/xformers), [detectron2](https://github.com/facebookresearch/detectron2), [mmcv](https://github.com/open-mmlab/mmcv), [mmdet](https://github.com/open-mmlab/mmdetection), [mmseg](https://github.com/open-mmlab/mmsegmentation), [ViT-Adapter](https://github.com/czczup/ViT-Adapter), [detrex](https://github.com/IDEA-Research/detrex), and [rotary-embedding-torch](https://github.com/lucidrains/rotary-embedding-torch).